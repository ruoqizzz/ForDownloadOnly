{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# load some utilities (for loading MNIST and plotting)\n",
    "# also imports most Python modules\n",
    "%run utils.py\n",
    "\n",
    "# load MNIST training and test data sets\n",
    "train_dataset, test_dataset = MNIST_datasets()\n",
    "\n",
    "# load tensors with all MNIST images and labels\n",
    "train_images, train_labels = MNIST()\n",
    "test_images, test_labels = MNIST(test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0ovZ1JKxOB6"
   },
   "source": [
    "# Variational Autoencoder\n",
    "\n",
    "Is it reasonable to assume that we can generate images similar to the ones in the MNIST data set by a linear transformation of low dimensional latent variables?\n",
    "\n",
    "Let us reflect on the limitations of PCA and PPCA. In both models, data is encoded in a lower dimensional space by a linear (or rather affine) transformation, and the lower dimensional representation can be decoded\n",
    "by another affine function. One implication is that relative distances are preserved between the original space and the lower-dimensional latent space: if two data samples are \"close\" to each other, then also their latent encodings are relatively \"close\" to each other, and similarly two latent encodings that are \"close\" to each other are decoded to two relatively \"close\" data points. This affects our ability to reconstruct images from their encodings and to generate new images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Answer Question 4.7 in the lab instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0ovZ1JKxOB6"
   },
   "source": [
    "Let us consider a more flexible non-linear model that is given by \n",
    "\\begin{align*}\n",
    "  p(\\mathbf{x} \\,|\\, \\mathbf{z}) &= \\mathcal{N}\\left(\\mathbf{x}; f(\\mathbf{z}; \\boldsymbol{\\phi}), \\sigma^2 \\mathbf{I}_{784}\\right), \\\\\n",
    "  p(\\mathbf{z}) &= \\mathcal{N}(\\mathbf{z}; 0, \\mathbf{I}_2),\n",
    "\\end{align*}\n",
    "where $\\sigma^2 > 0$ and $f(\\cdot; \\boldsymbol{\\phi}) \\colon \\mathbb{R}^2 \\to \\mathbb{R}^{784}$ is a nonlinear function with parameters $\\boldsymbol{\\phi}$. In this lab we model $f(\\cdot; \\boldsymbol{\\phi})$ by a neural network with parameters $\\boldsymbol{\\phi}$, but other models could be used equally well.\n",
    "\n",
    "This nonlinear model looks similar to the PPCA model above. However, in contrast to the PPCA model, for most classes of functions $f(\\cdot; \\boldsymbol{\\phi})$ the marginal distribution of $\\mathbf{x}$ is not a normal distribution anymore, and typically there exists not even a closed form expression for its density.\n",
    "Thus usually we can not learn the parameters $\\boldsymbol{\\xi} = (\\boldsymbol{\\phi}, \\sigma^2)$ in the same straightforward way as for the PPCA model by minimizing the negative log-likelihood with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0ovZ1JKxOB6"
   },
   "source": [
    "## Training: First approach (\"naive\")\n",
    "\n",
    "If there exists no closed-form expression for $p(\\mathbf{x}; \\boldsymbol{\\xi})$,\n",
    "alternatively one could approximate the integral\n",
    "\\begin{equation*}\n",
    "  p(\\mathbf{x}; \\boldsymbol{\\xi}) = \\int p(\\mathbf{x} \\,|\\, \\mathbf{z}; \\boldsymbol{\\xi}) p(\\mathbf{z}) \\,\\mathrm{d}\\mathbf{z}\n",
    "\\end{equation*}\n",
    "with the Monte Carlo method by a finite sum, i.e., one could work with the\n",
    "estimation\n",
    "\\begin{equation}\\label{eq:MC}\n",
    "  p(\\mathbf{x}; \\boldsymbol{\\xi}) \\approx \\frac{1}{K} \\sum_{n=1}^K p(\\mathbf{x} \\,|\\, \\mathbf{z}_n; \\boldsymbol{\\xi}),\n",
    "\\end{equation}\n",
    "where $\\mathbf{z}_1, \\ldots, \\mathbf{z}_K$ are i.i.d. samples of $\\mathbf{z}$. Thus for our nonlinear model we would obtain\n",
    "\\begin{equation*}\n",
    "  \\begin{split}\n",
    "  \\log p(\\mathbf{x}; \\boldsymbol{\\xi}) &\\approx \\log{\\left(\\sum_{n=1}^K \\mathcal{N}(\\mathbf{x}; f(\\mathbf{z}_n; \\boldsymbol{\\phi}), \\sigma^2 \\mathbf{I}_{784}) \\right)} - \\log K \\\\\n",
    "  &= \\log{\\left({(2\\pi \\sigma^2)}^{-392} \\sum_{n=1}^K \\exp{\\left(-\\frac{1}{2\\sigma^2} {\\|\\mathbf{x} - f(\\mathbf{z}_n; \\boldsymbol{\\phi})\\|}_2^2 \\right)}\\right)} - \\log K \\\\\n",
    "  &= - 392 (\\log \\sigma^2 + \\log{(2\\pi)}) + \\log{\\left(\\sum_{n=1}^K \\exp{\\left(-\\frac{1}{2\\sigma^2} {\\|\\mathbf{x} - f(\\mathbf{z}_n; \\boldsymbol{\\phi})\\|}_2^2 \\right)}\\right)} - \\log K.\n",
    "  \\end{split}\n",
    "\\end{equation*}\n",
    "Hence similar to the PPCA model, with a training data set $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N$ we can try to minimize the cost function\n",
    "\\begin{equation*}\n",
    "  J(\\boldsymbol{\\xi}) = 392 \\log \\sigma^2 - \\frac{1}{N} \\sum_{n'=1}^N \\log{\\left(\\sum_{n=1}^K \\exp{\\left(-\\frac{1}{2\\sigma^2} {\\|\\mathbf{x}_{n'} - f(\\mathbf{z}_n; \\boldsymbol{\\phi})\\|}_2^2 \\right)}\\right)}\n",
    "\\end{equation*}\n",
    "using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0C4SOa9dO4W"
   },
   "source": [
    "We now consider the following nonlinear model. The function $f(\\cdot; \\boldsymbol{\\phi})$ is modeled by a shallow neural network. The `decode` function outputs the representative decoding $f(\\mathbf{z}; \\boldsymbol{\\phi})$ for a batch of encodings $\\mathbf{z}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "orjYAtIzvfh_"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NonLinearModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NonLinearModel, self).__init__()\n",
    "\n",
    "        # linear parts of the nonlinear function f\n",
    "        self.decoder_fc1 = nn.Linear(2, 400)\n",
    "        self.decoder_fc2 = nn.Linear(400, 784)\n",
    "\n",
    "        # logarithm of variance sigma^2\n",
    "        self.logsigma2 = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def decode(self, z):\n",
    "        h1 = F.relu(self.decoder_fc1(z))\n",
    "        return self.decoder_fc2(h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Read through the definition of the nonlinear model and try to understand how $f(\\cdot; \\boldsymbol{\\phi})$ is defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by implementing the cost function that we derived above. One important part in the evaluation of the cost function is the sampling of $\\mathbf{z}_1, \\ldots, \\mathbf{z}_K$ and the computation of the decodings $f(\\mathbf{z}_i; \\boldsymbol{\\phi})$.\n",
    "\n",
    "## Task 3\n",
    "\n",
    "Implement the following function `sample_decode(model, K)` that returns a matrix of size $K \\times 784$ whose rows are the decodings $f(\\mathbf{z}_i; \\boldsymbol{\\phi}) \\in \\mathbb{R}^{784}$ of samples $\\mathbf{z}_1, \\ldots, \\mathbf{z}_K \\in \\mathbb{R}^2$ from $\\mathcal{N}(\\boldsymbol{0}, \\mathbf{I}_2)$, where $\\boldsymbol{\\phi}$ are the parameters of the nonlinear model `model`.\n",
    "\n",
    "*Hint*: You can sample a PyTorch matrix of size $n \\times m$ with standard normally distributed entries with [`torch.randn(n, m)`](https://pytorch.org/docs/stable/torch.html#torch.randn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_decode(model, K):\n",
    "    # sample z\n",
    "    # WRITE YOUR CODE HERE\n",
    "    \n",
    "    # compute f(z)\n",
    "    # WRITE YOUR CODE HERE\n",
    "    \n",
    "    return # WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-_JxZyUTvhgH"
   },
   "source": [
    "We use the function `sample_decode` to evaluate the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OhLBuG5-4vjH"
   },
   "outputs": [],
   "source": [
    "def cost_function(X, model, K):\n",
    "    # decodings f(z) of samples z\n",
    "    fz = sample_decode(model, K)\n",
    "    \n",
    "    # compute y with y[j, i] = - 1/(2 * sigma^2) * || x_j - f(z_i)) ||^2_2\n",
    "    y = - 0.5 * torch.exp(-model.logsigma2) * (X.view(-1, 1, 784) - fz.view(1, -1, 784)).pow(2).sum(dim=2)\n",
    "    \n",
    "    # compute loss\n",
    "    return 392 * model.logsigma2 - torch.logsumexp(y, dim=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DhCu1G_qww7f"
   },
   "source": [
    "We train the nonlinear model with stochastic gradient descent using batches of 500 images of the MNIST training data set for 20 epochs. In every iteration we use $K=5$ randomly sampled latent vectors $\\mathbf{z}_n$ to evaluate the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630
    },
    "colab_type": "code",
    "id": "iTgSecXmJGud",
    "outputId": "628ecbe7-eb9e-4643-a43b-273c2bc9aa10"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# define the data loaders\n",
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=500, shuffle=True)\n",
    "test_data = torch.utils.data.DataLoader(test_dataset, batch_size=500)\n",
    "\n",
    "# define the model\n",
    "model = NonLinearModel()\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# track the training and test loss\n",
    "training_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# optimize parameters for 20 epochs\n",
    "for i in range(20):\n",
    "\n",
    "    # for each minibatch\n",
    "    for x, _ in train_data:\n",
    "\n",
    "        # evaluate the cost function on the training data set\n",
    "        loss = cost_function(x, model, 5)\n",
    "\n",
    "        # update the statistics\n",
    "        training_loss.append(loss.item())\n",
    "        test_loss.append(float('nan'))\n",
    "\n",
    "        # perform backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a gradient descent step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # reset the gradient information\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # evaluate the model after every epoch\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # evaluate the cost function on the test data set\n",
    "        accumulated_loss = 0\n",
    "        for x, _ in test_data:\n",
    "            loss = cost_function(x, model, 5)\n",
    "            accumulated_loss += loss.item()\n",
    "            \n",
    "        # update the statistics\n",
    "        test_loss[-1] = accumulated_loss / len(test_data)\n",
    "            \n",
    "    print(f\"Epoch {i + 1:2d}: training loss {training_loss[-1]: 9.3f}, \"\n",
    "          f\"test loss {test_loss[-1]: 9.3f}\")\n",
    "        \n",
    "# plot loss\n",
    "plt.figure()\n",
    "iterations = np.arange(1, len(training_loss) + 1)\n",
    "plt.scatter(iterations, training_loss, label='training loss')\n",
    "plt.scatter(iterations, test_loss, label='test loss')\n",
    "plt.legend()\n",
    "plt.xlabel('iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "Read through and try to understand the implementation of the training procedure above. How does it differ from the implementation of the training procedure of the PPCA model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IhzPickAcSCg"
   },
   "source": [
    "We can use the trained model to generate new images, in the same way as we did with the PPCA model. Again we sample 25 encodings $\\mathbf{z}_1, \\ldots, \\mathbf{z}_{25}$ from $\\mathcal{N}(\\boldsymbol{0}, \\mathbf{I}_{2})$, and plot their representative decodings $f(\\mathbf{z}_n; \\boldsymbol{\\phi})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "colab_type": "code",
    "id": "HW0XywWGcbxb",
    "outputId": "1f6d0a29-2485-4190-b015-84ec368396b5"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  x = sample_decode(model, 25)\n",
    "\n",
    "plot_images(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmfRoFI4tZcF"
   },
   "source": [
    "## Training: second approach (variational autoencoder)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "There are at least two problems with the nonlinear model and our training procedure so far:\n",
    "- As mentioned in [Doersch's\n",
    "tutorial](https://arxiv.org/pdf/1606.05908.pdf), for most samples of\n",
    "$\\mathbf{z}$, $p(\\mathbf{x} \\,|\\, \\mathbf{z}; \\boldsymbol{\\xi})$ is almost zero. Hence these\n",
    "terms will not contribute much to the estimation of $p(\\mathbf{x}; \\boldsymbol{\\xi})$,  which can slow down the training procedure.\n",
    "- An even more fundamental problem with the nonlinear model is the fact that\n",
    "usually we can not compute an analytical expression for\n",
    "$p(\\mathbf{z} \\,|\\, \\mathbf{x}; \\boldsymbol{\\xi})$. Thus we can not encode\n",
    "data in the lower dimensional latent space and analyze its structure.\n",
    "\n",
    "The main idea of a so-called variational autotoencoder (VAE) is to resolve the first issue by attempting to obtain samples\n",
    "$\\mathbf{z}$ for which $p(\\mathbf{x} \\,|\\, \\mathbf{z}; \\boldsymbol{\\xi})$ is large.\n",
    "\n",
    "### ELBO\n",
    "\n",
    "Let us define an encoding distribution $q(\\mathbf{z}; \\mathbf{x}, \\boldsymbol{\\zeta})$ that may depend on $\\mathbf{x}$ and some parameters $\\boldsymbol{\\zeta}$. As shown in the lecture, then we have\n",
    "\\begin{equation*}\n",
    "    \\log p(\\mathbf{x}; \\boldsymbol{\\xi}) =\n",
    "    \\mathbb{E}_{\\mathbf{z} \\sim q(\\mathbf{z}; \\mathbf{x}, \\boldsymbol{\\zeta})}\\big[\\log p(\\mathbf{x} \\,|\\, \\mathbf{z}; \\boldsymbol{\\xi})\\big]\n",
    "    + \\mathrm{KL}\\big[q(\\mathbf{z}; \\mathbf{x}, \\boldsymbol{\\zeta}) \\,\\big\\|\\, p(\\mathbf{z} \\,|\\,\\mathbf{x}; \\boldsymbol{\\xi})\\big] - \\mathrm{KL}\\big[q(\\mathbf{z}; \\mathbf{x}, \\boldsymbol{\\zeta}) \\,\\big\\|\\, p(\\mathbf{z}; \\boldsymbol{\\xi})\\big],\n",
    "\\end{equation*}\n",
    "for all $\\mathbf{x}$ (we ignore here that the KL-divergence $\\mathrm{KL}(p \\,\\|\\, q)$ is only defined if $q(x) = 0$ implies $p(x) = 0$).\n",
    "\n",
    "The KL divergence of two distributions is always non-negative, and zero if and only if the two distributions are equal. Hence we have for all $\\mathbf{x}$\n",
    "\\begin{equation*}\n",
    "  \\log p(\\mathbf{x}; \\boldsymbol{\\xi}) \\geq\n",
    "  \\mathbb{E}_{\\mathbf{z} \\sim q(\\mathbf{z}; \\mathbf{x}, \\boldsymbol{\\zeta})}\\big[\\log p(\\mathbf{x} \\,|\\, \\mathbf{z}; \\boldsymbol{\\xi})\\big]\n",
    "  - \\mathrm{KL}\\big[q(\\mathbf{z}; \\mathbf{x}, \\boldsymbol{\\zeta}) \\,\\big\\|\\, p(\\mathbf{z}; \\boldsymbol{\\xi})\\big]\n",
    "\\end{equation*}\n",
    "with equality if and only if $q(\\mathbf{z}; \\mathbf{x}, \\boldsymbol{\\zeta})$ is equal to\n",
    "the distribution $p(\\mathbf{z} \\,|\\, \\mathbf{x}; \\boldsymbol{\\xi})$. Since the\n",
    "right-hand side of this inequality is a lower bound of the evidence $\\log p(\\mathbf{x}; \\boldsymbol{\\xi})$, it is called evidence lower bound (ELBO).\n",
    "\n",
    "Remember that we know $p(\\mathbf{z}; \\boldsymbol{\\xi}) = \\mathcal{N}(\\mathbf{z}; \\boldsymbol{0}, \\mathbf{I}_2)$ from the model specification. Moreover, $q(\\mathbf{z}; \\mathbf{x}, \\boldsymbol{\\zeta})$ is an\n",
    "arbitrary distribution that we can define in such a way that we can\n",
    "compute $\\mathrm{KL}\\big[q(\\mathbf{z}; \\mathbf{x}, \\boldsymbol{\\zeta}) \\,\\big\\|\\, p(\\mathbf{z}; \\boldsymbol{\\xi})\\big]$\n",
    "analytically. From Question 3.5 in the lab instructions, we know that we obtain\n",
    "\\begin{equation*}\n",
    "    \\mathrm{KL}\\big[q(\\mathbf{z}; \\mathbf{x}, \\boldsymbol{\\zeta}) \\,\\|\\, p(\\mathbf{z}; \\boldsymbol{\\xi})\\big] = \\frac{1}{2} \\left(\\sum_{n=1}^2 \\sigma^2_n(\\mathbf{x}; \\boldsymbol{\\zeta}) + \\|\\mu(\\mathbf{x}; \\boldsymbol{\\zeta})\\|^2_2 - 2 - \\sum_{n=1}^2 \\log \\sigma^2_n(\\mathbf{x}; \\boldsymbol{\\zeta})\\right),\n",
    "\\end{equation*}\n",
    "if we choose the encoding distribution\n",
    "\\begin{equation}\\label{eq:encoder}\n",
    "  q(\\mathbf{z}; \\mathbf{x}, \\boldsymbol{\\zeta}) = \\mathcal{N}(\\mathbf{z}; \\mu(\\mathbf{x}; \\boldsymbol{\\zeta}), \\mathrm{diag}(\\sigma^2_1(\\mathbf{x}; \\boldsymbol{\\zeta}), \\sigma^2_2(\\mathbf{x}; \\boldsymbol{\\zeta}))),\n",
    "\\end{equation}\n",
    "where $\\mu(\\cdot; \\boldsymbol{\\zeta}) \\colon \\mathbb{R}^{784} \\to \\mathbb{R}^2$ defines the mean of the normal distribution and $\\sigma^2_1(\\mathbf{x}; \\boldsymbol{\\zeta})$ and $\\sigma^2_2(\\mathbf{x}; \\boldsymbol{\\zeta})$ are the entries on the diagonal of the covariance matrix.\n",
    "\n",
    "Thus for this encoding distribution the ELBO is\n",
    "\\begin{equation}\n",
    " \\mathbb{E}_{\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{z}; \\mu(\\mathbf{x}; \\boldsymbol{\\zeta}), \\mathrm{diag}(\\sigma^2_1(\\mathbf{x}; \\boldsymbol{\\zeta}), \\sigma^2_2(\\mathbf{x}; \\boldsymbol{\\zeta})))}\\big[\\log p(\\mathbf{x} \\,|\\, \\mathbf{z}; \\boldsymbol{\\xi})\\big]\n",
    " - \\frac{1}{2} \\left(\\sum_{n=1}^2 \\sigma^2_n(\\mathbf{x}; \\boldsymbol{\\zeta}) + \\|\\mu(\\mathbf{x}; \\boldsymbol{\\zeta})\\|^2_2 - 2 - \\sum_{n=1}^2 \\log \\sigma^2_n(\\mathbf{x}; \\boldsymbol{\\zeta})\\right),\n",
    "\\end{equation}\n",
    "which we can approximate with the Monte Carlo estimate\n",
    "\\begin{equation}\n",
    "  \\frac{1}{K} \\sum_{n=1}^K \\log p(\\mathbf{x} \\,|\\, \\mathbf{z}_n; \\boldsymbol{\\xi}) - \\frac{1}{2} \\left(\\sum_{n=1}^2 \\sigma^2_n(\\mathbf{x}; \\boldsymbol{\\zeta}) + \\|\\mu(\\mathbf{x}; \\boldsymbol{\\zeta})\\|^2_2 - 2 - \\sum_{n=1}^2 \\log \\sigma^2_n(\\mathbf{x}; \\boldsymbol{\\zeta})\\right),\n",
    "\\end{equation}\n",
    "where $\\mathbf{z}_1, \\ldots, \\mathbf{z}_K$ are independent samples from $q(\\mathbf{z}; \\mathbf{x}, \\boldsymbol{\\zeta})= \\mathcal{N}(\\mathbf{z}; \\mu(\\mathbf{x}; \\boldsymbol{\\zeta}), \\mathrm{diag}(\\sigma^2_1(\\mathbf{x}; \\boldsymbol{\\zeta}), \\sigma^2_2(\\mathbf{x}; \\boldsymbol{\\zeta})))$.  The fundamental difference to the Monte Carlo estimation of the log-likelihood above is that before we had to sample from $p(\\mathbf{z}; \\boldsymbol{\\xi}) = \\mathcal{N}(\\mathbf{z}; \\boldsymbol{0}, \\mathbf{I}_2)$ whereas now we sample them from the encoding distribution $q$. Thus hopefully we can tune the parameters $\\boldsymbol{\\zeta}$ of the encoding distribution such that $p(\\mathbf{x} \\,|\\, \\mathbf{z}_n; \\boldsymbol{\\xi})$ increases, i.e., that it becomes more likely to reconstruct $\\mathbf{x}$ from the samples $\\mathbf{z}_n$.\n",
    "\n",
    "This observation motivates the idea of maximizing the ELBO by training parameters $\\boldsymbol{\\xi}$ and $\\boldsymbol{\\zeta}$ simultaneously instead of maximizing the evidence $\\log p(\\mathbf{x}; \\boldsymbol{\\xi})$ by training only $\\boldsymbol{\\xi}$. If the encoding distribution is flexible enough, we might even be able to obtain the distribution $p(\\mathbf{z} \\,|\\, \\mathbf{x}; \\boldsymbol{\\xi})$, in which case the ELBO is actually\n",
    "equal to the evidence.\n",
    "\n",
    "Additionally, if we manage that distribution $q$ becomes equal to the distribution $p(\\mathbf{z} \\,|\\, \\mathbf{x}; \\boldsymbol{\\xi})$ (or at least close to it), we have found a way to encode our data: for a given data sample $\\mathbf{x}$, we can just sample the latent encoding from $q(\\mathbf{z}; \\mathbf{x}, \\boldsymbol{\\zeta})$. So by introducing the encoding distribution and maximizing the ELBO, we might be able to solve both problems of the nonlinear model.\n",
    "\n",
    "### Cost function\n",
    "\n",
    "In the same way, for a set of training data $\\mathbf{x}_1, \\ldots, \\mathbf{x}_N$ we obtain\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "    \\log p(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N; \\boldsymbol{\\xi}) &= \\sum_{n=1}^N \\log p(\\mathbf{x}_n; \\boldsymbol{\\xi}) \\\\\n",
    "    &\\geq \\sum_{n=1}^N \\mathbb{E}_{\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{z}; \\mu(\\mathbf{x}_n; \\boldsymbol{\\zeta}), \\mathrm{diag}(\\sigma^2_1(\\mathbf{x}_n; \\boldsymbol{\\zeta}), \\sigma^2_2(\\mathbf{x}_n; \\boldsymbol{\\zeta})))}\\big[\\log p(\\mathbf{x}_n \\,|\\, \\mathbf{z}; \\boldsymbol{\\xi})\\big] \\\\\n",
    "   &\\quad - \\frac{1}{2} \\sum_{n=1}^N \\left(\\sum_{i=1}^2 \\sigma^2_i(\\mathbf{x}_n; \\boldsymbol{\\zeta}) + \\|\\mu(\\mathbf{x}_n; \\boldsymbol{\\zeta})\\|^2_2 - 2 - \\sum_{i=1}^2 \\log \\sigma^2_i(\\mathbf{x}_n; \\boldsymbol{\\zeta})\\right),\n",
    "    \\end{split}\n",
    "\\end{equation*}\n",
    "and thus we can estimate the joint ELBO by\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    "     & \\sum_{n=1}^N \\frac{1}{K} \\sum_{n'=1}^K \\log p(\\mathbf{x}_n \\,|\\, \\mathbf{z}_{n,n'}; \\boldsymbol{\\xi})\n",
    "  - \\frac{1}{2} \\sum_{n=1}^N \\left(\\sum_{i=1}^2 \\sigma^2_i(\\mathbf{x}_n; \\boldsymbol{\\zeta}) + \\|\\mu(\\mathbf{x}_n; \\boldsymbol{\\zeta})\\|^2_2 - 2 - \\sum_{i=1}^2 \\log \\sigma^2_i(\\mathbf{x}; \\boldsymbol{\\zeta})\\right) \\\\\n",
    "     ={}& - \\frac{1}{2}\\left(784 N \\left(\\log{(2\\pi)} + \\log{\\sigma^2})\\right) + \\frac{1}{\\sigma^2}\\sum_{n=1}^N \\frac{1}{K} \\sum_{n'=1}^K \\|f(\\mathbf{z}_{n,n'}; \\boldsymbol{\\xi}) - \\mathbf{x}_n\\|^2_2 \\right) \\\\\n",
    "     & - \\frac{1}{2} \\sum_{n=1}^N \\left(\\sum_{i=1}^2 \\sigma^2_i(\\mathbf{x}_n; \\boldsymbol{\\zeta}) + \\|\\mu(\\mathbf{x}_n; \\boldsymbol{\\zeta})\\|^2_2 - 2 - \\sum_{i=1}^2 \\log \\sigma^2_i(\\mathbf{x}_n; \\boldsymbol{\\zeta})\\right),\n",
    "     \\end{split}\n",
    "\\end{equation*}\n",
    "where $\\mathbf{z}_{n,1}, \\ldots, \\mathbf{z}_{n,N}$ are independent samples of the encoding distribution $\\mathcal{N}(\\mathbf{z}; \\mu(\\mathbf{x}_n; \\boldsymbol{\\zeta}), \\mathrm{diag}(\\sigma^2_1(\\mathbf{x}_n; \\boldsymbol{\\zeta}), \\sigma^2_2(\\mathbf{x}_n; \\boldsymbol{\\zeta})))$.\n",
    "\n",
    "Since we use a stochastic optimization algorithm anyway, we choose $K = 1$, i.e., we use only one Monte Carlo sample for each training data sample. After neglecting additive constant terms and scaling by $N/2$, we obtain the cost function\n",
    "\\begin{equation*}\n",
    " J(\\boldsymbol{\\xi}, \\boldsymbol{\\zeta}) = 784 \\log{\\sigma^2} + \\frac{1}{N \\sigma^2} \\sum_{n=1}^N  \\|f(\\mathbf{z}_n; \\boldsymbol{\\xi}) - \\mathbf{x}_n\\|^2_2 + \\frac{1}{N} \\sum_{n=1}^N \\left(\\sum_{i=1}^2 \\sigma^2_i(\\mathbf{x}_n; \\boldsymbol{\\zeta}) + \\|\\mu(\\mathbf{x}_n; \\boldsymbol{\\zeta})\\|^2_2 - \\sum_{i=1}^2 \\log \\sigma^2_i(\\mathbf{x}_n; \\boldsymbol{\\zeta})\\right),\n",
    "\\end{equation*}\n",
    "where $\\mathbf{z}_n$ are samples from $\\mathcal{N}(\\mathbf{z}; \\mu(\\mathbf{x}_n; \\boldsymbol{\\zeta}), \\mathrm{diag}(\\sigma^2_1(\\mathbf{x}_n; \\boldsymbol{\\zeta}), \\sigma^2_2(\\mathbf{x}_n; \\boldsymbol{\\zeta})))$. We will minimize this cost function with stochastic gradient descent.\n",
    "\n",
    "### Reparameterization trick\n",
    "\n",
    "However, there is one problem: it is not clear how to differentiate through the sampling operation of $\\mathbf{z}_n$ with respect to the parameters $\\boldsymbol{\\zeta}$ that determine the mean and variance of the distribution we sample from. We can generate samples from an arbitrary normal distribution by an affine transformation of standard normally distributed samples. Thus we can rewrite the cost function as\n",
    "\\begin{equation*}\n",
    "    \\begin{split}\n",
    " J(\\boldsymbol{\\xi}, \\boldsymbol{\\zeta}) &= 784 \\log{\\sigma^2} + \\frac{1}{N \\sigma^2} \\sum_{n=1}^N  \\|f(\\mu(\\mathbf{x}_n; \\boldsymbol{\\zeta}) + \\mathrm{diag}(\\sigma_1(\\mathbf{x}_n; \\boldsymbol{\\zeta}), \\sigma_2(\\mathbf{x}_n; \\boldsymbol{\\zeta})) \\boldsymbol{\\epsilon}_n; \\boldsymbol{\\xi}) - \\mathbf{x}_n\\|^2_2 \\\\\n",
    " &\\quad + \\frac{1}{N} \\sum_{n=1}^N \\left(\\sum_{i=1}^2 \\sigma^2_i(\\mathbf{x}_n; \\boldsymbol{\\zeta}) + \\|\\mu(\\mathbf{x}_n; \\boldsymbol{\\zeta})\\|^2_2 - \\sum_{i=1}^2 \\log \\sigma^2_i(\\mathbf{x}_n; \\boldsymbol{\\zeta})\\right),\n",
    " \\end{split}\n",
    "\\end{equation*}\n",
    "where $\\boldsymbol{\\epsilon}_n$ are samples from the normal distribution $\\mathcal{N}(\\boldsymbol{0}, \\mathbf{I}_2)$. Rewriting the cost function in this form is known as reparameterization trick and allows us to differentiate the cost function with respect to $\\boldsymbol{\\zeta}$ in a straightforward way, since the samples $\\boldsymbol{\\epsilon}_n$ do not depend on the parameters anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1NQHSjZqS1x2"
   },
   "source": [
    "We now consider the following nonlinear model, in which in addition to the nonlinear model above we also implement an encoding distribution. The mean and the logarithm of the diagonal entries of the covariance matrix of the encoding distribution are modeled by a shallow neural network, and the `encode` function returns them for a batch of inputs. As above, the `decode` function outputs the representative decoding for a batch of encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VCjfJcOvDjjD"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # linear parts of the nonlinear encoder\n",
    "        self.encoder_fc1 = nn.Linear(784, 400)\n",
    "        self.encoder_fc2_mean = nn.Linear(400, 2)\n",
    "        self.encoder_fc2_logsigma2 = nn.Linear(400, 2)\n",
    "\n",
    "        # linear parts of the nonlinear function f\n",
    "        self.decoder_fc1 = nn.Linear(2, 400)\n",
    "        self.decoder_fc2 = nn.Linear(400, 784)\n",
    "\n",
    "        # logarithm of variance sigma^2\n",
    "        self.logsigma2 = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.encoder_fc1(x))\n",
    "        return self.encoder_fc2_mean(h1), self.encoder_fc2_logsigma2(h1)\n",
    "\n",
    "    def decode(self, z):\n",
    "        h1 = F.relu(self.decoder_fc1(z))\n",
    "        return self.decoder_fc2(h1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5\n",
    "\n",
    "Read through and try to understand the definition of the nonlinear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed above, one term of the cost function is the term\n",
    "\\begin{equation}\n",
    "\\frac{1}{N} \\sum_{n=1}^N \\left(\\sum_{i=1}^2 \\sigma^2_i(\\mathbf{x}_n; \\boldsymbol{\\zeta}) + \\|\\mu(\\mathbf{x}_n; \\boldsymbol{\\zeta})\\|^2_2 - \\sum_{i=1}^2 \\log \\sigma^2_i(\\mathbf{x}_n; \\boldsymbol{\\zeta})\\right),\n",
    "\\end{equation}\n",
    "that originates from the KL divergence expression in the ELBO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "\n",
    "Implement the function `kl_divergence_term(Z_mu, Z_logsigma2)` that evaluates the expression above, where the rows of `Z_mu` are $\\mu(\\mathbf{x}_n; \\boldsymbol{\\zeta})$ and the rows of `Z_logsigma2` are $\\begin{bmatrix} \\log \\sigma^2_1(\\mathbf{x}_n; \\boldsymbol{\\zeta}) & \\log \\sigma^2_2(\\mathbf{x}_n; \\boldsymbol{\\zeta})\\end{bmatrix}^\\intercal$.\n",
    "\n",
    "*Hint*: You should probably use the PyTorch functions [`torch.mean`](https://pytorch.org/docs/stable/torch.html#torch.mean), [`torch.sum`](https://pytorch.org/docs/stable/torch.html#torch.sum), [`torch.exp`](https://pytorch.org/docs/stable/torch.html#torch.exp), and [`torch.pow`](https://pytorch.org/docs/stable/torch.html#torch.pow) in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_term(Z_mu, Z_logsigma2):\n",
    "    # compute the average KL divergence to the standard normal\n",
    "    # distribution, neglecting additive constant terms\n",
    "    return # WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ME785_-hCzS_"
   },
   "source": [
    "We make use of the `kl_divergence_term` function in our implementation of the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vpb-CqiMANjo"
   },
   "outputs": [],
   "source": [
    "def cost_function(X, model):\n",
    "    # compute mean and log variance of the normal distribution of\n",
    "    # the encoding z of input X\n",
    "    Z_mu, Z_logsigma2 = model.encode(X)\n",
    "\n",
    "    # compute the average KL divergence to the prior standard normal\n",
    "    # distribution of z, neglecting constant terms\n",
    "    # expected log p(x | z) + C\n",
    "    kl = kl_divergence_term(Z_mu, Z_logsigma2)\n",
    "\n",
    "    # sample z\n",
    "    Z = Z_mu + torch.randn(Z_mu.size()) * torch.exp(0.5 * Z_logsigma2)\n",
    "\n",
    "    # compute the representative decoding of z\n",
    "    X_decoding = model.decode(Z)\n",
    "\n",
    "    # compute negative average evidence of the input x, neglecting additive constant terms\n",
    "    neg_log_evidence = 784 * model.logsigma2 + \\\n",
    "        torch.sum((X_decoding - X).pow(2) * torch.exp(- model.logsigma2), dim=1).mean()\n",
    "\n",
    "    return neg_log_evidence + kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PG-BUpp6EsAR"
   },
   "source": [
    "Now we can train the nonlinear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 260
    },
    "colab_type": "code",
    "id": "KS_0buw0H_Fz",
    "outputId": "948d87ef-e377-4b75-f8ce-7128094f4c79"
   },
   "outputs": [],
   "source": [
    "# define the data loaders\n",
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=500, shuffle=True)\n",
    "test_data = torch.utils.data.DataLoader(test_dataset, batch_size=500)\n",
    "\n",
    "# define the model\n",
    "model = VAE()\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# track the training and test loss\n",
    "training_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# optimize parameters for 20 epochs\n",
    "for i in range(20):\n",
    "\n",
    "    # for each minibatch\n",
    "    for x, _ in train_data:\n",
    "\n",
    "        # evaluate the cost function on the training data set\n",
    "        loss = cost_function(x, model)\n",
    "\n",
    "        # update the statistics\n",
    "        training_loss.append(loss.item())\n",
    "        test_loss.append(float('nan'))\n",
    "\n",
    "        # perform backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a gradient descent step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # reset the gradient information\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # evaluate the model after every epoch\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # evaluate the cost function on the test data set\n",
    "        accumulated_loss = 0\n",
    "        for x, _ in test_data:\n",
    "            loss = cost_function(x, model)\n",
    "            accumulated_loss += loss.item()\n",
    "            \n",
    "        # update statistics\n",
    "        test_loss[-1] = accumulated_loss / len(test_data)\n",
    "            \n",
    "    print(f\"Epoch {i + 1:2d}: training loss {training_loss[-1]: 9.3f}, \"\n",
    "          f\"test loss {test_loss[-1]: 9.3f}\")\n",
    "        \n",
    "# plot loss\n",
    "plt.figure()\n",
    "iterations = np.arange(1, len(training_loss) + 1)\n",
    "plt.scatter(iterations, training_loss, label='training loss')\n",
    "plt.scatter(iterations, test_loss, label='test loss')\n",
    "plt.legend()\n",
    "plt.xlabel('iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7\n",
    "\n",
    "Read through and try to understand the implementation of the training procedure above. How does it differ from the implementation of the training procedure of the nonlinear model without encoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_w-OSf1YEBV"
   },
   "source": [
    "In contrast to the regular nonlinear model without encoder, we can encode the images of the MNIST data set and repeat the same analysis as for the PPCA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cx1jpTpSZb87"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # compute representative encoding of the training images\n",
    "    train_encoding, _ = model.encode(train_images)\n",
    "\n",
    "    # compute representative encoding of the test images\n",
    "    test_encoding, _ = model.encode(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kIEaGRc1aKs6"
   },
   "source": [
    "We visualize the encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "uDOSMStZaRvg",
    "outputId": "f862ebc9-385a-49e2-efef-06173cd3f9a0"
   },
   "outputs": [],
   "source": [
    "plot_encoding((train_encoding, train_labels), (test_encoding, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the digits 0, 1, $\\ldots$, 9 we compute the average representation in the latent space by taking the mean of the encodings of the MNIST training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean encoding\n",
    "train_mean_encodings = mean_encodings(train_encoding, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize their location in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_encoding((train_encoding, train_labels), (test_encoding, test_labels),\n",
    "              train_mean_encodings, annotate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can also decode the latent encodings with our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean images\n",
    "with torch.no_grad():\n",
    "    train_mean_images = model.decode(train_mean_encodings)\n",
    "\n",
    "plot_images(train_mean_images, torch.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get a feeling for the distribution in the latent space by defining and analysing a whole grid of encodings, spanned by the mean encodings of the digits \"0\" and \"9\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute grid of latent vectors\n",
    "zgrid = create_grid(train_mean_encodings[0], train_mean_encodings[9])\n",
    "\n",
    "# visualize it\n",
    "plot_encoding((train_encoding, train_labels), (test_encoding, test_labels), zgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show the corresponding decoded images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "colab_type": "code",
    "id": "W9yozZEakgHs",
    "outputId": "c27c9c95-de74-4c62-d6e3-ce1957bfd701"
   },
   "outputs": [],
   "source": [
    "# compute mean images\n",
    "with torch.no_grad():\n",
    "    xgrid = model.decode(zgrid)\n",
    "\n",
    "plot_images(xgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dUjex5_klsWO"
   },
   "source": [
    "As in the previous parts of the lab session, we also compare the test images with their reconstructions to see how much information we lose by encoding the MNIST images in a two-dimensional space. We plot a set of images and their reconstructions, and compute the average squared reconstruction error as a more objective measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "colab_type": "code",
    "id": "w01vtgxJmKQU",
    "outputId": "d9143914-6c5f-4949-e636-2e1dad855c1a"
   },
   "outputs": [],
   "source": [
    "# compute reconstruction\n",
    "with torch.no_grad():\n",
    "    test_reconstruction = model.decode(test_encoding)\n",
    "\n",
    "# compute average squared reconstruction error\n",
    "sqerr = (test_images - test_reconstruction).pow(2).sum(dim=1).mean()\n",
    "print(f\"Average squared reconstruction error: {sqerr}\")\n",
    "\n",
    "plot_reconstruction(test_images, test_reconstruction, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8\n",
    "\n",
    "Now we have performed exactly the same analysis as for the regular and the probabilistic PCA. Compare your results and answer Questions 4.8, 4.9, and 4.10 in the lab instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iY4kAEAFYGXv"
   },
   "source": [
    "We can also generate new MNIST-like images in the same way as for the nonlinear model without encoder. Again we sample 25 vectors $\\mathbf{z}_1, \\ldots, \\mathbf{z}_{25}$ from $\\mathcal{N}(0, \\mathbf{I}_{2})$, and plot the representative decoding $f(\\mathbf{z}_n; \\boldsymbol{\\phi})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "colab_type": "code",
    "id": "QcanQkO-Cd6p",
    "outputId": "0a383781-ca21-46d3-c2ac-f2ee7484e8e9"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  x = sample_decode(model, 25)\n",
    "\n",
    "plot_images(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have trained a variational autoencoder that allows us to generate MNIST-like images. Adding a nonlinearity and an encoder seems to improve the quality of the samples. However, we also notice that the model is not perfect. Many further modifications of the decoder and encoder models are possible and could potentially improve the sampler. For instance, the dimension of the latent space can be increased (then the information loss by encoding the images in the latent space should be reduced). Alternatively the nonlinear decoder model can be changed: a more flexible model with increased number of layers in the neural network or a diagonal (or even full) covariance matrix could be used, or the outputs could be restricted to values between 0 and 1 (since we represent MNIST images as vectors with entries between 0 and 1)."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "colab": {
   "name": "VAE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
