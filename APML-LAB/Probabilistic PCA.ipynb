{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROfdN0x2XNPW"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# load some utilities (for loading MNIST and plotting)\n",
    "# also imports most Python modules\n",
    "%run utils.py\n",
    "\n",
    "# load MNIST training and test data sets\n",
    "train_dataset, test_dataset = MNIST_datasets()\n",
    "\n",
    "# tensors with all MNIST images and labels\n",
    "train_images, train_labels = MNIST()\n",
    "test_images, test_labels = MNIST(test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXGqSi1BbMzX"
   },
   "source": [
    "# Probabilistic PCA\n",
    "\n",
    "\n",
    "As before, let $\\mathbf{x} \\in \\mathbb{R}^{784}$ represent a $28 \\times 28$-pixel grayscale image and $\\mathbf{z} \\in \\mathbb{R}^2$ be a two-dimensional latent variable.\n",
    "\n",
    "Instead of regular PCA we know use [probabilistic PCA](https://rss.onlinelibrary.wiley.com/doi/10.1111/1467-9868.00196) as a model for dimensionality reduction and feature extraction, since probabilistic PCA can be altered and extended quite easily and allows a probabilistic interpretation of the encodings, which helps us with generating new MNIST-like images. In our setting, the probabilistic PCA model is given by \n",
    "\\begin{align*}\n",
    "  p(\\mathbf{x} \\,|\\, \\mathbf{z}) &= \\mathcal{N}\\left(\\mathbf{x}; \\mathbf{W}\\mathbf{z} + \\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}_{784}\\right), \\\\\n",
    "  p(\\mathbf{z}) &= \\mathcal{N}(\\mathbf{z}; \\boldsymbol{0}, \\mathbf{I}_2),\n",
    "\\end{align*}\n",
    "with parameters $\\mathbf{W} \\in \\mathbb{R}^{784 \\times 2}$, $\\boldsymbol{\\mu} \\in \\mathbb{R}^{784}$, and $\\sigma^2 > 0$.\n",
    "\n",
    "In Question 3.3 in the lab document, you showed that for the probabilistic PCA model\n",
    "\\begin{equation*}\n",
    "    p(\\mathbf{x}) = \\mathcal{N}(\\mathbf{x}; \\boldsymbol{\\mu}, \\mathbf{C}),\n",
    "\\end{equation*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "    \\mathbf{C} = \\mathbf{W} \\mathbf{W}^\\intercal + \\sigma^2 \\mathbf{I}_{784}.\n",
    "\\end{equation*}\n",
    "Thus the log-likelihood of i.i.d. data samples $\\{\\mathbf{x}_1, \\ldots, \\mathbf{x}_N\\}$ is given by\n",
    "\\begin{equation*}\n",
    "    \\log p(\\mathbf{x}_1, \\ldots, \\mathbf{x}_N; \\mathbf{W}, \\boldsymbol{\\mu}, \\sigma^2) = - \\frac{N}{2}(784 \\log{(2\\pi)} + \\log{|\\mathbf{C}|}) - \\frac{1}{2} \\sum_{n=1}^N {(\\mathbf{x}_n - \\boldsymbol{\\mu})}^\\intercal {\\mathbf{C}}^{-1} {(\\mathbf{x}_n - \\boldsymbol{\\mu})}.\n",
    "\\end{equation*}\n",
    "\n",
    "[Tipping and Bishop](https://rss.onlinelibrary.wiley.com/doi/10.1111/1467-9868.00196) actually derived an expression for the maximum likelihood solution of $\\mathbf{W}$, $\\boldsymbol{\\mu}$, and $\\sigma^2$. However, in this lab we do not try to obtain the maximum likelihood solution with the algorithms suggested in their original work. Instead, since the log-likelihood is differentiable with respect to the parameters, we will optimize the parameters with gradient descent, similar to the linear regression example in the Jupyter notebook \"Introduction to PyTorch\".\n",
    "\n",
    "We try to minimize the cost function\n",
    "\\begin{equation*}\n",
    "  J(\\mathbf{W}, \\boldsymbol{\\mu}, \\sigma^2) = \\log{|\\mathbf{C}|} + \\frac{1}{N} \\sum_{n=1}^N {(\\mathbf{x}_n - \\boldsymbol{\\mu})}^\\intercal {\\mathbf{C}}^{-1} {(\\mathbf{x}_n - \\boldsymbol{\\mu})},\n",
    "\\end{equation*}\n",
    "where we neglected all additive constant terms of the log-likelihood and scaled it by $N / 2$.\n",
    "\n",
    "It is computationally much cheaper to work with the\n",
    "matrix\n",
    "\\begin{equation*}\n",
    "\\mathbf{M} = \\mathbf{W}^\\intercal \\mathbf{W} + \\sigma^2 \\mathbf{I}_2 \\in \\mathbb{R}^{2 \\times 2}\n",
    "\\end{equation*}\n",
    "than with the matrix $\\mathbf{C} \\in \\mathbb{R}^{784 \\times 784}$.\n",
    "Actually one can exploit the special structure of the matrix\n",
    "$\\mathbf{C}$ and use the [matrix determinant lemma](https://en.wikipedia.org/wiki/Matrix_determinant_lemma#Generalization) and the [Woodbury matrix identity](https://en.wikipedia.org/wiki/Woodbury_matrix_identity) to show that\n",
    "\\begin{equation*}\n",
    "      J(\\mathbf{W}, \\boldsymbol{\\mu}, \\sigma^2) = 782 \\log \\sigma^2 +\n",
    "      \\log{|\\mathbf{M}|} +\n",
    "      \\frac{1}{N\\sigma^2} \\sum_{n=1}^N \\left(\\|\\mathbf{x}_n - \\boldsymbol{\\mu}\\|_2^2 -\n",
    "        {(\\mathbf{x}_n - \\boldsymbol{\\mu})}^\\intercal \\mathbf{W}\n",
    "        {\\mathbf{M}}^{-1} \\mathbf{W}^\\intercal\n",
    "        (\\mathbf{x}_n - \\boldsymbol{\\mu}) \\right).\n",
    "\\end{equation*}\n",
    "\n",
    "We start by implementing the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72Dm5wSH_USn"
   },
   "source": [
    "## Task 1\n",
    "\n",
    "The following Python function `cost_function` will be used to evaluate the cost function $J$ for a given data set\n",
    "\\begin{equation*}\n",
    "\\mathbf{X} = \\begin{bmatrix} \\mathbf{x}_1^\\intercal \\\\ \\vdots \\\\ \\mathbf{x}_N^\\intercal \\end{bmatrix}\n",
    "\\end{equation*}\n",
    "and paramters $\\mathbf{W}$, $\\boldsymbol{\\mu}$, and $\\log \\sigma^2$. To enforce $\\sigma^2 > 0$, we optimize the real-valued parameter $\\log \\sigma^2$ instead of $\\sigma^2$. Read through and try to understand the existing implementation. Add a final line to the function that computes and returns the value of the cost function by making use of the already computed values and matrices.\n",
    "\n",
    "*Hint*: Use the PyTorch functions [`torch.pow`](https://pytorch.org/docs/stable/torch.html#torch.pow) for taking the power of each element in a PyTorch tensor, and [`torch.sum`](https://pytorch.org/docs/stable/torch.html#torch.sum) and [`torch.mean`](https://pytorch.org/docs/stable/torch.html#torch.mean) for computing the sum and the mean of a PyTorch tensor (it is possible to specify the dimension along which the operation should be performed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGISRv6T0fiH"
   },
   "outputs": [],
   "source": [
    "def cost_function(X, W, mu, logsigma2):\n",
    "    # compute matrix Xshifted with rows (x_n - mu)^T\n",
    "    # note: mu is defined as a row vector\n",
    "    Xshifted = X - mu\n",
    "    \n",
    "    # compute matrix Y with rows (x_n - mu)^T * W\n",
    "    Y = Xshifted.mm(W)\n",
    "\n",
    "    # compute matrix M = W^T * W + sigma^2 I\n",
    "    sigma2 = logsigma2.exp()\n",
    "    M = W.t().mm(W) + torch.diagflat(sigma2.expand(2))\n",
    "\n",
    "    # compute the log-determinant of M\n",
    "    Mlogdet = M.logdet()\n",
    "\n",
    "    # compute the inverse of M\n",
    "    Minverse = M.inverse()\n",
    "\n",
    "    # compute vector C with C[n] = (x_n - mu)^T * W * M^(-1) * W^T * (x_n - mu)\n",
    "    C = Y.mm(Minverse).mm(Y.t()).diagonal()\n",
    "    \n",
    "    # put everything together and compute loss\n",
    "    return # WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in particular for higher-dimensional latent spaces one would want to the [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition) of $\\mathbf{M}$ to speed up the computation of $\\log{|\\mathbf{M}|}$ and ${(\\mathbf{x}_n - \\boldsymbol{\\mu})}^\\intercal \\mathbf{W} {\\mathbf{M}}^{-1} \\mathbf{W}^\\intercal (\\mathbf{x}_n - \\boldsymbol{\\mu})$ (actually, that is exactly how it is implemented in [PyTorch](https://pytorch.org/docs/stable/_modules/torch/distributions/lowrank_multivariate_normal.html)).\n",
    "\n",
    "We train the model with [Adam](https://arxiv.org/abs/1412.6980), a gradient-based optimization algorithm with adaptive learning rates for different parameters. Instead of evaluating the gradient based on all images in the training data set\n",
    "in every step, we compute it from a randomly chosen subset of the training data, a so-called minibatch. The main idea is that the gradients computed from a large enough random subset of the data should be roughly similar to the gradient evaluated on the whole data set, but by using a subset of the data set the computation time can be improved.\n",
    "\n",
    "In PyTorch data loaders are used for iterating through minibatches. The following code snippet creates data loaders of the MNIST training and test data sets that return minibatches of 500 images and their corresponding labels upon iteration. The samples in the training data set are shuffled whereas the samples in the test data set are always returned in the same order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWPurwTXf3Nv"
   },
   "outputs": [],
   "source": [
    "# define data loaders\n",
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=500, shuffle=True)\n",
    "test_data = torch.utils.data.DataLoader(test_dataset, batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DesCP5q9gJmc"
   },
   "source": [
    "The next code snippet shows a complete implementation of the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630
    },
    "colab_type": "code",
    "id": "Cry2BVXTgXv-",
    "outputId": "388d2e83-43ab-4f45-d972-b867b13de0d4"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# define the data loaders\n",
    "train_data = torch.utils.data.DataLoader(train_dataset, batch_size=500, shuffle=True)\n",
    "test_data = torch.utils.data.DataLoader(test_dataset, batch_size=500)\n",
    "\n",
    "# define the initial model parameters\n",
    "W = torch.randn((784, 2), requires_grad=True)\n",
    "mu = torch.zeros(1, 784, requires_grad=True)\n",
    "logsigma2 = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = optim.Adam([W, mu, logsigma2], lr=0.01)\n",
    "\n",
    "# track the training and test loss\n",
    "training_loss = []\n",
    "test_loss = []\n",
    "\n",
    "# optimize parameters for 20 epochs\n",
    "for i in range(20):\n",
    "\n",
    "    # for each minibatch\n",
    "    for x, _ in train_data:\n",
    "\n",
    "        # evaluate the cost function on the training data set\n",
    "        loss = cost_function(x, W, mu, logsigma2)\n",
    "\n",
    "        # update the statistics\n",
    "        training_loss.append(loss.item())\n",
    "        test_loss.append(float('nan'))\n",
    "\n",
    "        # perform backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a gradient descent step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # reset the gradient information\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # evaluate the model after every epoch\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # evaluate the cost function on the test data set\n",
    "        accumulated_loss = 0\n",
    "        for x, _ in test_data:\n",
    "            loss = cost_function(x, W, mu, logsigma2)\n",
    "            accumulated_loss += loss.item()\n",
    "            \n",
    "        # update the statistics\n",
    "        test_loss[-1] = accumulated_loss / len(test_data)\n",
    "            \n",
    "    print(f\"Epoch {i + 1:2d}: training loss {training_loss[-1]: 9.3f}, \"\n",
    "          f\"test loss {test_loss[-1]: 9.3f}\")\n",
    "        \n",
    "# plot the tracked statistics\n",
    "plt.figure()\n",
    "iterations = np.arange(1, len(training_loss) + 1)\n",
    "plt.scatter(iterations, training_loss, label='training loss')\n",
    "plt.scatter(iterations, test_loss, label='test loss')\n",
    "plt.legend()\n",
    "plt.xlabel('iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "\n",
    "Read through and try to understand the implementation of the training procedure above. How does it differ from the implementation of the training procedure in the linear regression example? Answer Question 4.3 in the lab instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We inspect the trained model and perform a similar analysis as for regular PCA. Since we do not update the parameters anymore, no gradients have to be computed in the following sections. We can prevent PyTorch from tracking all our computations and building computational graphs by changing the attribute `requires_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.requires_grad = False\n",
    "mu.requires_grad = False\n",
    "logsigma2.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWueTFkfMVvt"
   },
   "source": [
    "In Question 3.4 in the lab instructions we showed that the distribution of the latent representation $\\mathbf{z}$ conditioned on an image $\\mathbf{x}$ is also Gaussian and given by\n",
    "\\begin{equation*}\n",
    "    p(\\mathbf{z} \\,|\\, \\mathbf{x}) = \\mathcal{N}\\left(\\mathbf{z}; \\mathbf{M}^{-1} \\mathbf{W}^\\intercal (\\mathbf{x} - \\boldsymbol{\\mu}), \\sigma^2 \\mathbf{M}^{-1}\\right),\n",
    "\\end{equation*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "\\mathbf{M} = \\mathbf{W}^\\intercal \\mathbf{W} + \\sigma^2 \\mathbf{I}_2.\n",
    "\\end{equation*}\n",
    "We can use this result to encode the MNIST images in the lower-dimensional latent space. In contrast to regular PCA there exists not one unique representation of an image in the latent space, but instead each image gives rise to a whole distribution of representations in the latent space. Here we use the mean\n",
    "\\begin{equation*}\n",
    "\\mathbf{M}^{-1} \\mathbf{W}^\\intercal (\\mathbf{x} - \\boldsymbol{\\mu})\n",
    "\\end{equation*}\n",
    "of the normal distribution as encoding. Alternatively, one could sample from the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DoJ2RGL6Lz0x"
   },
   "outputs": [],
   "source": [
    "# compute M = W^T * W + sigma^2 * I\n",
    "M = W.t().mm(W) + torch.diagflat(logsigma2.exp().expand(2))\n",
    "\n",
    "# compute the inverse of M\n",
    "Minv = torch.inverse(M)\n",
    "\n",
    "# compute encoding of the training images\n",
    "train_encoding = (train_images - mu).mm(W).mm(Minv)\n",
    "\n",
    "# compute encoding of the test images\n",
    "test_encoding = (test_images - mu).mm(W).mm(Minv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QKDEn_RDM7HJ"
   },
   "source": [
    "We visualize the distribution of the encodings in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "_kRdXm2nNOhE",
    "outputId": "b4d928e9-4296-49c5-c57d-9df1b8e8396c"
   },
   "outputs": [],
   "source": [
    "plot_encoding((train_encoding, train_labels), (test_encoding, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpdo01OfZiXX"
   },
   "source": [
    "For each of the digits $0, 1, \\ldots, 9$ we compute the average representation in the latent space by taking the mean of the encodings of the MNIST training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "colab_type": "code",
    "id": "gaQ3kMUkoYel",
    "outputId": "bce2e63b-283f-459e-a312-90274f90fd82"
   },
   "outputs": [],
   "source": [
    "# compute mean encoding\n",
    "train_mean_encodings = mean_encodings(train_encoding, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us show where these average representations are located in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_encoding((train_encoding, train_labels), (test_encoding, test_labels),\n",
    "              train_mean_encodings, annotate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4VOBobMjLYm"
   },
   "source": [
    "We can decode the representations in the latent space, according to the model definition\n",
    "\\begin{align*}\n",
    "  p(\\mathbf{x} \\,|\\, \\mathbf{z}) &= \\mathcal{N}\\left(\\mathbf{x}; \\mathbf{W}\\mathbf{z} + \\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}_{784}\\right), \\\\\n",
    "  p(\\mathbf{z}) &= \\mathcal{N}(\\mathbf{z}; \\boldsymbol{0}, \\mathbf{I}_2).\n",
    "\\end{align*}\n",
    "Again in contrast to regular PCA, the model provides us with a whole distribution of possible decodings. Analogously to the encoding discussed above, here we take the mean\n",
    "\\begin{equation*}\n",
    "\\overline{\\mathbf{x}} = \\mathbf{W}\\mathbf{z} + \\boldsymbol{\\mu}\n",
    "\\end{equation*}\n",
    "of the normal distribution as representative decoding. Of course, alternatively we could sample from the normal distribution defined by the probabilistic PCA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "\n",
    "Complete the following code snippet by implementing the representative decoding of the `train_mean_encodings`.\n",
    "\n",
    "*Hint*: It might be easier to work with the equivalent formulation $\\overline{\\mathbf{x}}^\\intercal = \\mathbf{z}^\\intercal \\mathbf{W}^\\intercal + \\boldsymbol{\\mu}^\\intercal$. Note that `mu` is defined as the row vector $\\boldsymbol{\\mu}^\\intercal$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute mean images\n",
    "train_mean_images = # WRITE YOUR CODE HERE\n",
    "\n",
    "plot_images(train_mean_images, torch.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a whole grid of points in the latent space that is spanned by the mean encoding for digit \"0\" and digit \"9\" in the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "colab_type": "code",
    "id": "pirRa49By8rq",
    "outputId": "6b1c0e22-7b29-4163-8231-b3bf913d2ab5"
   },
   "outputs": [],
   "source": [
    "# compute grid of latent vectors\n",
    "zgrid = create_grid(train_mean_encodings[0], train_mean_encodings[9])\n",
    "\n",
    "# visualize it\n",
    "plot_encoding((train_encoding, train_labels), (test_encoding, test_labels), zgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we decode the latent representations and plot the resulting images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "colab_type": "code",
    "id": "pirRa49By8rq",
    "outputId": "6b1c0e22-7b29-4163-8231-b3bf913d2ab5"
   },
   "outputs": [],
   "source": [
    "# compute mean images\n",
    "xgrid = zgrid.mm(W.t()) + mu\n",
    "\n",
    "plot_images(xgrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "llasDHlNbMze"
   },
   "source": [
    "As in regular PCA, it is interesting to compare an image $\\mathbf{x}$ with its reconstruction $\\mathbf{\\tilde{x}}$, to see how much information about $\\mathbf{x}$ is kept/lost by the reduction of $\\mathbf{x}$ to its representation in the latent space. As we discussed above, different encodings and decodings could be considered in the probabilistic settings, leading to different reconstructions. To be consistent with our analysis above, we take the representative decoding of the representative encoding used above as reconstruction.\n",
    "\n",
    "We compute the reconstructions of the images in the MNIST test data set and plot them alongside the original images. Moreover, again we evaluate the average squared reconstruction error as an objective measure for the quality of the reconstructions. As a side remark, the reconstruction that we use is [not optimal in the squared reconstruction error sense](https://rss.onlinelibrary.wiley.com/doi/10.1111/1467-9868.00196), and hence the average squared reconstruction error could be improved by defining it in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611
    },
    "colab_type": "code",
    "id": "52Tq0RftpsdR",
    "outputId": "3729a405-3349-43ef-bdec-c1cfec6229ed"
   },
   "outputs": [],
   "source": [
    "# compute reconstruction\n",
    "test_reconstruction = test_encoding.mm(W.t()) + mu\n",
    "\n",
    "# compute average squared reconstruction error\n",
    "sqerr = (test_images - test_reconstruction).pow(2).sum(dim=1).mean()\n",
    "print(f\"Average squared reconstruction error: {sqerr}\")\n",
    "\n",
    "plot_reconstruction(test_images, test_reconstruction, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4\n",
    "\n",
    "Now we have performed exactly the same analysis for both the regular and the probabilistic PCA. Compare your results and answer Questions 4.4, 4.5, and 4.6 in the lab instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regular PCA, a priori no distribution of the latent representations is specified by the model. Hence it is difficult to know which encodings might produce reasonably looking images, which makes sampling new encodings and hence new images challenging. In contrast, probabilistic PCA defines the marginal distribution $p(\\mathbf{z})= \\mathcal{N}(\\mathbf{z}; \\boldsymbol{0}, \\mathbf{I}_2)$. Thus we can generate new MNIST-like images by sampling encodings from this distribution and decoding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(25, 2)\n",
    "x = z.mm(W.t()) + mu\n",
    "\n",
    "plot_images(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this part of the lab session, we replaced regular PCA with its probabilistic formulation. Although the PPCA model is more difficult to train than regular PCA, we are can generate new MNIST-like images in a general way now! Unfortunately, to be honest, the generated images do not look great. We will try to improve this in the next part of the lab."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "colab": {
   "name": "Probabilistic PCA.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
